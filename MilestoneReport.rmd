---
title: "Milestone Report"
author: "Brandon Clark"
date: "6/26/2020"
output: html_document
---

# Coursera Data Science Milestone Report

## Introduction
Below is the milestone report for the Coursera Data Science Specialization capstone project.  We were given 3 text files containing collected text from Blogs, Twitter posts, and News articles.  I will be using these datasets to create a predictive text model to provide text input recommendations on mobile devices.  

## Assumptions
Data is downloaded and unzipped in active R directory

## Load Packages and Data

```{r echo=FALSE}
curDir = 'C:/Users/bclar/OneDrive/Documents/Data Science Specialization/10 - Capstone Project'
setwd(curDir)
```

```{r warning=FALSE}
library(tm)
library(dplyr)
library(wordcloud)
library(textreg)
library(quanteda)

blogsFile <- file( paste0( curDir, '/datasets/en_US/en_US.blogs.txt' ), open = "rb")
en_US.blogs = readLines( blogsFile, encoding = "UTF-8", skipNul=TRUE )

twitterFile <- file( paste0( curDir, '/datasets/en_US/en_US.twitter.txt' ), open = "rb" )
en_US.twitter = readLines( twitterFile, encoding = "UTF-8", skipNul=TRUE )

newsFile <- file( paste0( curDir, '/datasets/en_US/en_US.news.txt' ), open = "rb")
en_US.news = readLines( newsFile, encoding = "UTF-8", skipNul=TRUE)

#remove non eneglish characters
en_US.blogs <- iconv(en_US.blogs, "latin1", "ASCII", sub="")
en_US.twitter<- iconv(en_US.twitter, "latin1", "ASCII", sub="")
en_US.news <- iconv(en_US.news, "latin1", "ASCII", sub="")
```

## Get Word and Line Counts
We will use a helper function to get word and line counts

```{r warning=FALSE}
f.word.count <- function(my.list) { sum(stringr::str_count(my.list, "\\S+")) }
list <- list(blog = en_US.blogs , twitter = en_US.twitter, news = en_US.news)

df <- data.frame(source = c("blog", "twitter", "news"), line.count = NA, word.count = NA)

df$line.count <- sapply(list, length)
df$word.count <- sapply(list, f.word.count)

par(mfrow=c(1,2))
barplot(height = df$line.count, names.arg = df$source , ylab = 'line count')
barplot(height = df$word.count, names.arg = df$source , ylab = 'word count')
```

## Sample Data
The datasets are quite large.  We will use binomial sampling to get a random 5% sample of each.  We will also aggregate the data sources into one character class
```{r warning=FALSE}
percent <- 0.05

sampleFunction <- function(data, percent)
{
  return(data[as.logical(rbinom(length(data),2,percent))])
}

en_US.blogs   <- sampleFunction(en_US.blogs, percent)
en_US.twitter   <- sampleFunction(en_US.twitter, percent)
en_US.news   <- sampleFunction(en_US.news, percent)

allSources<-c(en_US.blogs,en_US.twitter,en_US.news)
```

## Convert to Corpus and Clean
Each datasource will need to be converted into a corpus.  We will then clean each corpus by removing punctuation, numbers, converting to all lower case, removing stop words, and stripping white space.  

NOTE: Stop words are removed for exploratory analysis.  They will be an important part of the final alogithm and will be included later.

We will also create dataframes to store word frequency.
```{r warning=FALSE}
en_US.blogsCorp <- Corpus(VectorSource(en_US.blogs))
en_US.twitterCorp <- Corpus(VectorSource(en_US.twitter))
en_US.newsCorp <- Corpus(VectorSource(en_US.news))
allSourcesCorp <- Corpus(VectorSource(allSources))

allSourcesCorpCleaned <- allSourcesCorp
allSourcesCorpCleaned <- tm_map(allSourcesCorpCleaned, removePunctuation)  
allSourcesCorpCleaned <- tm_map(allSourcesCorpCleaned, removeNumbers)     
allSourcesCorpCleaned <- tm_map(allSourcesCorpCleaned, tolower)     
allSourcesCorpCleaned <- tm_map(allSourcesCorpCleaned, removeWords, stopwords("english"))  
allSourcesCorpCleaned <- tm_map(allSourcesCorpCleaned, stripWhitespace)   

allSourcesDfm <- dfm(corpus(allSourcesCorpCleaned), verbose = FALSE)
allSourcesD <- textstat_frequency(allSourcesDfm)
colnames(allSourcesD) <- c('word','allSourcesFreq','allSourcesrank')
allSourcesD<-allSourcesD[,1:3]

en_US.blogsCleaned <- en_US.blogsCorp
en_US.blogsCleaned <- tm_map(en_US.blogsCleaned, removePunctuation)  
en_US.blogsCleaned <- tm_map(en_US.blogsCleaned, removeNumbers)     
en_US.blogsCleaned <- tm_map(en_US.blogsCleaned, tolower)     
en_US.blogsCleaned <- tm_map(en_US.blogsCleaned, removeWords, stopwords("english"))  
en_US.blogsCleaned <- tm_map(en_US.blogsCleaned, stripWhitespace)   

blogsDfm <- dfm(corpus(en_US.blogsCleaned), verbose = FALSE)
blogsD <- textstat_frequency(blogsDfm)
colnames(blogsD) <- c('word','blogsFreq','blogsRank')
blogsD<-blogsD[,1:3]

en_US.twitterCleaned <- en_US.twitterCorp
en_US.twitterCleaned <- tm_map(en_US.twitterCleaned, removePunctuation)  
en_US.twitterCleaned <- tm_map(en_US.twitterCleaned, removeNumbers)     
en_US.twitterCleaned <- tm_map(en_US.twitterCleaned, tolower)     
en_US.twitterCleaned <- tm_map(en_US.twitterCleaned, removeWords, stopwords("english"))  
en_US.twitterCleaned <- tm_map(en_US.twitterCleaned, stripWhitespace) 

twitterDfm <- dfm(corpus(en_US.twitterCleaned), verbose = FALSE)
twitterD <- textstat_frequency(twitterDfm)
colnames(twitterD) <- c('word','twitterFreq','twitterRank')
twitterD<-twitterD[,1:3]

en_US.newsCleaned <- en_US.newsCorp
en_US.newsCleaned <- tm_map(en_US.newsCleaned, removePunctuation)  
en_US.newsCleaned <- tm_map(en_US.newsCleaned, removeNumbers)     
en_US.newsCleaned <- tm_map(en_US.newsCleaned, tolower)     
en_US.newsCleaned <- tm_map(en_US.newsCleaned, removeWords, stopwords("english"))  
en_US.newsCleaned <- tm_map(en_US.newsCleaned, stripWhitespace)   

newsDfm <- dfm(corpus(en_US.newsCleaned), verbose = FALSE)
newsD <- textstat_frequency(newsDfm)
colnames(newsD) <- c('word','newsFreq','newsRank')
newsD<-newsD[,1:3]

allSourcesDF <- allSourcesD
allSourcesDF <- left_join(allSourcesD, blogsD, by = 'word')
allSourcesDF <- left_join(allSourcesDF, newsD, by = 'word')
allSourcesDF <- left_join(allSourcesDF, twitterD, by = 'word')
head(allSourcesDF, n = 20)
```

## Wordcloud

```{r warning=FALSE}
par(mfrow=c(1,1))
set.seed(1234)

wordcloud(words = allSourcesDF$word, freq = allSourcesDF$allSourcesFreq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), main = 'All Data Sources')
```

## Plot Cumulative Word Frequency
The below plot shows that the majority of the words are in the most frequent and that there is a long tail of less frequently used words.  There are over 200000 words.  The top 1000 (removing stop words) make up 50% of the total word count!  The top 17000 make up 90%.
```{r warning=FALSE}
allSourcesDF$cumProportion <- cumsum( allSourcesDF$allSourcesFreq )/sum(allSourcesDF$allSourcesFreq )

par(mfrow=c(1,1))
plot1 <-plot( seq.int(nrow(allSourcesDF)) , cumsum( allSourcesDF$allSourcesFreq )/sum(allSourcesDF$allSourcesFreq ),
      xlab = "Distinct Word Count", ylab = "Cumulative Proportion", 
      main = "All Data Sources: Ordered Word Count, Cumulative Sum/Total Word Count") +
  abline(h = .9, col = "red", lty = 2, cex =  2) +
  abline(h = .5, col = "blue", lty = 2, cex = 2) +
  abline(v=nrow(allSourcesDF) , col = "green", lty = 2, cex = 2) +
  abline(v = nrow(allSourcesDF[which(allSourcesDF$cumProportion < .5),]), col = "blue", lty = 2, cex = 2) +
  abline(v = nrow(allSourcesDF[which(allSourcesDF$cumProportion < .9),]), col = "red", lty = 2, cex = 2) +
  text(x=0,y=.53, "50%", col = "blue", cex = 1) +
  text(x=0,y=.93, "90%", col = "red", cex = 1) +
  text(x=nrow(allSourcesDF[which(allSourcesDF$cumProportion < .9),]),y=0, nrow(allSourcesDF[which(allSourcesDF$cumProportion < .9),]), col = "red", cex = 1) +
  text(x=nrow(allSourcesDF[which(allSourcesDF$cumProportion < .5),]),y=0, nrow(allSourcesDF[which(allSourcesDF$cumProportion < .5),]), col = "blue", cex = 1) +
  text(x=nrow(allSourcesDF),y=0, nrow(allSourcesDF), col = "green", cex = 1) 
plot1
```

## Unigrams, Bigrams, Trigrams
N-grams will be an important part of the prediction alogithm.  We will look at words commonly occuring together

We will use the publicly shared ngram_tokenizer function to assist: https://github.com/zero323/r-snippets/blob/master/R/ngram_tokenizer.R

```{r echo=FALSE}
ngram_tokenizer <- function(n = 1L, skip_word_none = TRUE, skip_word_number = FALSE) {
  stopifnot(is.numeric(n), is.finite(n), n > 0)
  
  #' To avoid :: calls
  stri_split_boundaries <- stringi::stri_split_boundaries
  stri_join <- stringi::stri_join
  
  options <- stringi::stri_opts_brkiter(
    type="word", skip_word_none = skip_word_none, skip_word_number = skip_word_number
  )
  
  #' Tokenizer
  #' 
  #' @param x character
  #' @return character vector with n-grams
  function(x) {
    stopifnot(is.character(x))
    
    # Split into word tokens
    tokens <- unlist(stri_split_boundaries(x, opts_brkiter=options))
    len <- length(tokens)
    
    if(all(is.na(tokens)) || len < n) {
      # If we didn't detect any words or number of tokens is less than n return empty vector
      character(0)
    } else {
      sapply(
        1:max(1, len - n + 1),
        function(i) stri_join(tokens[i:min(len, i + n - 1)], collapse = " ")
      )
    }
  }
}
```
Unigrams
```{r warning=FALSE}
unigram.tokenizer <- ngram_tokenizer(1)
wordlist <- unigram.tokenizer(as.character(allSourcesCorpCleaned))
unigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(unigram.df) <- c("word","freq")
row.names(unigram.df) <- NULL
unigram.df <- unigram.df[with(unigram.df, order(-unigram.df$freq)),]
head(unigram.df, n=10)
```

Bigrams
```{r warning=FALSE}
bigram.tokenizer <- ngram_tokenizer(2)
wordlist <- bigram.tokenizer(as.character(allSourcesCorpCleaned))
bigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(bigram.df) <- c("word","freq")
row.names(unigram.df) <- NULL
bigram.df <- bigram.df[with(bigram.df, order(-bigram.df$freq)),]
head(bigram.df)
```

Trigrams
```{r warning=FALSE}
trigram.tokenizer <- ngram_tokenizer(3)
wordlist <- trigram.tokenizer(as.character(allSourcesCorpCleaned))
trigram.df <- data.frame(V1 = as.vector(names(table(unlist(wordlist)))), V2 = as.numeric(table(unlist(wordlist))))
names(trigram.df) <- c("word","freq")
row.names(trigram.df) <- NULL
trigram.df <- trigram.df[with(trigram.df, order(-trigram.df$freq)),]
head(trigram.df)
```

## Next Steps
N-grams will be a main piece of the prediction alogithm.  The last word (unigram) of an N-gram will split from the previous ones.  The most likely unigrams will be recommended.
- Bigrams will be separated into unigram/unigram
- Trigram will be separated into bigram/unigram
- This will continue until the largest N is reached

We will also look into models that weight different various datasources based on their predictive performance. 

The final model will be in a Shiny App

